\documentclass[PhD]{PHlab-thesis}
\reversemarginpar        % 避免錯誤；不關心邊註方向時可保留
\setlength{\marginparwidth}{0pt}   % 邊註寬度設 0
\setlength{\marginparsep}{0pt}     % 邊註與正文的間距也設 0
\addbibresource{thesis.bib}

\newcommand*\Department中文{資訊工程學系}
\newcommand*\Department英文{Department of Computer Science and Information Engineering}

\newcommand*\ThesisTitle中文{WebGPU 於高效能生物資訊計算：瀏覽器端 Pair-HMM Forward 演算法之優化}
\newcommand*\ThesisTitle英文{ WebGPU for High-Performance Bioinformatics Computing: Browser-Based Optimization of the Pair-HMM Forward Algorithm}

\newcommand*\Student中文{王尊緯}
\newcommand*\Student英文{Tsun-Wei Wang}

\newcommand*\Advisor中文{賀保羅}
\newcommand*\Advisor英文{Paul Horton}

%% 果有共同指導老師可以用:
%% \newcommand*\CoAdvisorA中文{}
%% \newcommand*\CoAdvisorA英文{}
%% \newcommand*\CoAdvisorB中文{}
%% \newcommand*\CoAdvisorB英文{}


\newcommand*\YearMonth英文{July, 2025}
\newcommand*\YearMonth中文{１１４年７月}

\pagestyle{fancy}% Use fancyhdr

\usepackage{fancyhdr}
\pagestyle{fancy}
\setlength{\headheight}{13.6pt}
\renewcommand{\chaptermark}[1]{\markboth{Chapter \thechapter.\ #1}{}}
\fancyhead{}

\fancyhead[C]{\small\MakeUppercase{\leftmark}}


\begin{document}


% ---------- Keywords ----------
\newcommand*\Keywords英文{WebGPU, Pair-Hidden Markov Model, Bioinformatics Acceleration}
\newcommand*\Keywords中文{瀏覽器 GPU 計算、Pair-Hidden Markov Model (Pair-HMM)、生物資訊加速}


% ---------- Abstract (English) ----------
\newcommand*\Abstract英文{%
As GPU acceleration gains traction in bioinformatics (Banerjee \emph{et al}., 2017; Liu \emph{et al}., 2021), conventional CUDA and OpenCL workflows still require vendor-specific driver installation and remain tethered to particular hardware, limiting online teaching and front-end clinical analysis. Standardized in 2024, \textbf{WebGPU} unifies Vulkan, Direct3D 12, and Metal under a single JavaScript API (W3C, 2024), offering three key advantages: no installation, cross-hardware portability, and on-device data residency. Using the compute-intensive Pair-Hidden Markov Model Forward (Pair-HMM Forward) algorithm (Durbin \emph{et al}., 1998) as a case study, we assess the performance and feasibility of this emerging framework.

Building on the open-source C++/CUDA implementation by Chou Yu-Chen (Chou, 2024), we first develop a WebGPU baseline. We then mitigate its principal bottlenecks—frequent CPU↔GPU round-trips and costly BindGroup reconstruction—by introducing (i) batched submission of a single \emph{CommandBuffer} and (ii) Dynamic Uniform Offsets, yielding an optimized variant termed \emph{WebGPU-Optimized}.

Benchmarks on three devices—NVIDIA RTX 2070 Super, Apple M1, and Intel UHD 620—across sequence lengths $10^{2}$–$10^{5}$ show that the optimized version attains speed-ups exceeding $100\times$ in the best case and reaches over 80 \% of CUDA's throughput, while maintaining relative log-likelihood error below $10^{-5}$ on all devices. Even without an NVIDIA GPU, our WebGPU implementation outperforms single-threaded C++ by one to two orders of magnitude.

These findings demonstrate that pure JavaScript and WGSL can execute the Pair-HMM Forward algorithm within seconds in a web browser. We contribute two browser-specific optimization strategies and provide detailed cross-hardware performance measurements, laying the groundwork for Web-native genomic analysis tools and advancing the democratization and real-time execution of bioinformatics workloads.
}

% ---------- Abstract (Chinese) ----------
\newcommand*\Abstract中文{%
隨著 GPU 加速在生物資訊領域日益普及（Banerjee 等，2017；Liu 等，2021），傳統 CUDA／OpenCL 工作流程仍需安裝廠商驅動程式，且受限於特定硬體，難以支援線上教學與前端臨床分析。2024 年標準化的 \textbf{WebGPU} 以單一 JavaScript API 統合 Vulkan、Direct3D 12 與 Metal（W3C，2024），具備「免安裝」、跨硬體可攜性與本機資料駐留三大優勢。本研究以運算密集的 Pair-Hidden Markov Model Forward（Pair-HMM Forward）演算法（Durbin 等，1998）為案例，評估該新框架的效能與可行性。

我們以周育晨（Chou，2024）公開的 C++／CUDA 原始碼為基準，首先實作 WebGPU Baseline；隨後針對 CPU↔GPU 往返頻繁與 BindGroup 重建耗時等瓶頸，依序導入「單一 \emph{CommandBuffer} 批次提交」與「Dynamic Uniform Offsets」，形成 \emph{WebGPU-Optimized}。

在 NVIDIA RTX 2070 Super、Apple M1 與 Intel UHD 620 三款裝置上，針對序列長度 $10^{2}$–$10^{5}$ 的測試顯示：優化版本在最佳情況下可獲得逾 $100\times$ 的加速，並達到 CUDA 效能的 80 \% 以上；三裝置的相對 log-likelihood 誤差皆低於 $10^{-5}$。即便無 NVIDIA GPU，本方法仍較單執行緒 C++ 提供一至二個數量級的加速效果。

研究結果證實，僅憑 JavaScript 與 WGSL，即能在瀏覽器中於秒級完成 Pair-HMM Forward 計算。我們提出兩項瀏覽器端專屬優化策略，並提供跨硬體之詳細效能量測，為 Web 原生基因體分析工具奠定基礎，推動生物資訊運算的民主化與即時化。
}





\newcommand*\Acknowledgements{%
在此特別感謝賀保羅教授以及在實驗室給予我協助與指導的同學：阮祈翰、楊祐昇、黃書堯、鄭驊軒、鄭煜醴等，在各階段實驗設計與資料收集上提供許多寶貴意見。

同時感謝林宜靜、賴威達、王晴文、許庸袁及陳竑曄，你們在討論分析與程式測試時的協作與支持，讓本研究得以順利完成。

最後，感謝所有關心及協助本研究的師長與同窗，謹此致上由衷謝意。 }



\input{frontmatter}% 封面頁, 口委中英文簽名單, 誌謝, 中英文摘要, 論文目錄, 圖表目錄


\renewcommand\nomgroup[1]{%
  \item[\bfseries
  \ifstrequal{#1}{A}{General}{%
  \ifstrequal{#1}{Z}{Gene/Protein Names}%
  }]}

% === A. General ===
\nomenclature[A]{HPC}{High-Performance Computing}
\nomenclature[A]{API}{Application Programming Interface}
\nomenclature[A]{BWA}{Burrows–Wheeler Aligner}
\nomenclature[A]{GATK}{Genome Analysis Toolkit}

% === B. Algorithm / Computational Model ===
\nomenclature[B]{HMM}{Hidden Markov Model}
\nomenclature[B]{SIMD}{Single Instruction Multiple Data}
\nomenclature[B]{FMA}{Fused Multiply-Add instruction}
\nomenclature[B]{SM}{Streaming Multiprocessor (CUDA core block)}
\nomenclature[B]{LUT}{Lookup Table}

% === C. GPU / WebGPU & Related Technologies ===
\nomenclature[C]{CUDA}{Compute Unified Device Architecture}
\nomenclature[C]{OpenCL}{Open Computing Language}
\nomenclature[C]{WASM}{WebAssembly}
\nomenclature[C]{WASM-SIMD}{SIMD extension of WebAssembly}
\nomenclature[C]{WGPU}{Cross-platform WebGPU API implementation}
\nomenclature[C]{ALU}{Arithmetic Logic Unit}
\nomenclature[C]{UMA}{Unified Memory Architecture}
\nomenclature[C]{DRAM}{Dynamic Random-Access Memory}
\nomenclature[C]{TFLOPS}{Tera Floating-Point Operations per Second}
\nomenclature[C]{SDK}{Software Development Kit}


\printnomenclature[5cm]

\newpage
\setcounter{page}{1}
\pagenumbering{arabic}

\chapter{Introduction}

\section{Background}
High-throughput sequencing (HTS), commonly known as next-generation sequencing (NGS), has driven an exponential growth in genomic data (Mardis, 2017). Such scale places unprecedented demands on computational throughput.  
The \textbf{Pair-Hidden Markov Model Forward algorithm} (Pair-HMM Forward), a core kernel for sequence alignment, genotype calling, and variant detection (Durbin \emph{et al}., 1998), is among the most compute-intensive steps in modern pipelines.

State-of-the-art workflows—GATK (McKenna \emph{et al}., 2010), Samtools (Li \emph{et al}., 2009), and BWA-MEM2 (Vasimuddin \emph{et al}., 2019)—are primarily written in C++ or Python and achieve acceleration via NVIDIA CUDA or OpenCL (Liu \emph{et al}., 2021; Banerjee \emph{et al}., 2017). Besides installing drivers, SDKs, and various dependencies, users are locked to specific GPU architectures. In classrooms or resource-constrained labs, the absence of high-end GPUs—or limited cloud quotas—often forces a CPU fallback, inflating cost and turnaround time.  
Cloud services alleviate local setup but introduce account management overhead, network latency, and concerns over sensitive-data exposure.

Since WebGPU reached Chrome stable in May 2024 (Google Chrome Developers, 2024), Firefox Nightly and Edge Dev have followed with experimental support. By mapping Vulkan, Direct3D 12, and Metal to a single JavaScript API inside the browser sandbox (W3C, 2024), WebGPU enables real-time parallel computation on NVIDIA, AMD, Intel, and Apple Silicon GPUs without any driver installation. This opens a new path to lowering the entry barrier for bioinformatics tools—particularly in teaching, clinical front-ends, and low-resource settings.

\section{Motivation and Objectives}
While WebGPU promises zero installation, cross-hardware portability, and on-device data residency, it was designed primarily for graphics and machine-learning inference. A dynamic-programming kernel such as Pair-HMM Forward encounters several obstacles:

\begin{itemize}
  \item \textit{API scheduling overhead} – Wavefronts must be processed sequentially; if each dispatch creates fresh command buffers and bind groups, CPU ↔ GPU round-trips accumulate rapidly.
  \item \textit{Lack of global synchronization} – Every wavefront depends on the previous one, yet WebGPU exposes only workgroup-level barriers and lacks CUDA-style kernel-return global sync (W3C, 2024).
  \item \textit{Absence of dedicated special-function units (SFUs)} – Pair-HMM issues many \texttt{log}/\texttt{exp} calls. CUDA's SFUs finish $\log_{2}$ in four cycles (NVIDIA, 2023), whereas WebGPU must approximate via alu + lut + fma, incurring a 2–4$\times$ latency penalty.
  \item \textit{High memory-access demand} – The DP matrix resides in a read–write storage buffer; WebGPU's default path bypasses L1 cache, so frequent global reads/writes incur heavy DRAM traffic (Liu \emph{et al}., 2021).
\end{itemize}

These limitations leave WebGPU's suitability for bioinformatics unverified. We therefore ask: \textit{Can a browser-resident WebGPU implementation execute Pair-HMM Forward with practical performance compared with CUDA?}

Our objectives are:
\begin{enumerate}
  \item identify the dominant bottlenecks when porting Pair-HMM Forward to WebGPU;
  \item design WebGPU-specific optimizations that mitigate those bottlenecks;
  \item benchmark performance and accuracy across heterogeneous GPUs.
\end{enumerate}

\section{Methods and Key Results}
Using the open-source C++/CUDA reference by Yu-Chen Chou \cite{chou2024} as baseline, we introduce two WebGPU-specific optimizations:

\begin{enumerate}
  \item \textit{Single-command-buffer batch submission} – Aggregate multiple wavefronts into one \verb|queue.submit()|, eliminating excessive API latency.
  \item \textit{Dynamic uniform offsets} – Store static parameters in a single uniform buffer and access them via dynamic offsets, omitting repeated UBO allocation and binding.
\end{enumerate}

On an NVIDIA RTX 2070 Super, across sequence lengths 100–100\,000, the optimized version accelerates the baseline by 6.8–142$\times$ and attains 11–84 \% of CUDA's throughput. On Apple M1 and Intel UHD 620, it delivers 4–463$\times$ speed-ups over single-threaded C++ when sequences are $\ge 1\,000$, while maintaining log-likelihood errors below $10^{-5}$.

\section{Conclusions and Contributions}
This study demonstrates that JavaScript plus WGSL can solve medium- to large-scale Pair-HMM Forward instances within seconds inside a browser sandbox. The two complementary browser-side optimizations effectively mitigate API, synchronization, and memory bottlenecks, and cross-vendor experiments on NVIDIA, Apple, and Intel GPUs confirm hardware agnosticism.  
Our findings pave the way for “open-the-browser-and-compute” genomic analysis and provide an empirical basis for future work on FP64 support and WASM-SIMD + WebGPU hybrid acceleration.




\chapter{Related Work}

\section{High-Performance Computing Requirements in Bioinformatics}

\subsection{Next-Generation Sequencing and Its Computational Challenges}
The rapid advance of next-generation sequencing (NGS) has driven genomic data volumes to grow exponentially (Mardis, 2017). According to Illumina, a NovaSeq X Plus equipped with a 25 B flow cell operating in dual-lane mode can produce approximately $5.2\times10^{10}$ paired-end reads ($2\times150$ bp) in a single 48-hour run—roughly $3.25\times10^{11}$ bases per hour (Illumina, 2024). Such throughput demands sequence-alignment and probabilistic computations that far exceed the capacity of traditional CPU-only architectures.

Alignment tools such as BWA (Li \& Durbin, 2010) and Bowtie (Langmead \emph{et al}., 2009) routinely process millions to billions of reads. Although their worst-case complexity is $O(NM)$, FM-index–based seeding and extension render the average cost nearly linear, $O(L)$, in read length $L$ (Ferragina \& Manzini, 2000). To sustain these workloads, researchers increasingly employ high-performance computing (HPC) resources—especially GPUs—to meet the near-real-time requirements of modern workflows (Liu \emph{et al}., 2021).

\subsection{The Central Role of the Pair-HMM Forward Algorithm}
The Pair-Hidden Markov Model (Pair-HMM) Forward algorithm is fundamental to sequence alignment and genotype inference, underpinning pipelines such as GATK (McKenna \emph{et al}., 2010) and Samtools (Li \emph{et al}., 2009). With time complexity $O(NM)$ and memory that can be reduced to $O(\max\{N,M\})$ (Durbin \emph{et al}., 1998; Banerjee \emph{et al}., 2017), it becomes the dominant bottleneck for long reads ($N\approx10^{4}$). Recent work shows that GPU parallelisation can shorten runtime dramatically—for example, Schmidt \emph{et al}. (2024) report reducing the analysis of $32\times12$ kb fragments on an NVIDIA RTX 4090 from hours to under three minutes. Nevertheless, the algorithm's performance remains highly sensitive to memory-access patterns and numerical precision, demanding hardware-aware optimisation.

\section{Conventional GPU Acceleration Frameworks: CUDA and OpenCL}

\subsection{CUDA in Bioinformatics}
NVIDIA's CUDA has become the de-facto standard for GPU computing in bioinformatics. Liu \emph{et al}. (2013) introduced CUDASW++ 3.0, accelerating the Smith–Waterman algorithm by $30$–$90\times$ over a single CPU core. Schmidt \emph{et al}. (2024) applied CUDA to Pair-HMM Forward, achieving high throughput in large-scale genotyping pipelines. However, CUDA is confined to NVIDIA hardware and requires driver plus toolkit installation, raising the barrier for non-expert users. Furthermore, kernels often need retuning for each micro-architecture (e.g., Ampere, Hopper), hindering portability.

\subsection{OpenCL's Cross-Platform Ambition}
OpenCL aspires to hardware-agnostic GPU acceleration across NVIDIA, AMD, and Intel devices. Stone \emph{et al}. (2010) demonstrated its scientific potential by accelerating molecular-dynamics simulations. Despite this promise, OpenCL is less prevalent in bioinformatics than CUDA, owing to uneven hardware support and a steeper learning curve. Klöckner \emph{et al}. (2012) observed that OpenCL's memory-management and synchronisation semantics vary widely between vendors, leading to unpredictable performance. In addition, its library ecosystem is less mature, further constraining adoption.

\subsection{Barriers and Limitations of Traditional Frameworks}
Although CUDA and OpenCL can deliver substantial speed-ups, both entail intricate setup—driver installation, SDK configuration, and dependency management—that hampers deployment in educational and clinical settings. Cloud GPUs (e.g., AWS, Google Cloud) reduce local overhead but introduce latency and data-privacy concerns (Krampis \emph{et al}., 2012). Moreover, these frameworks remain tightly coupled to specific hardware and offer limited cross-platform portability, restricting their usefulness on resource-constrained devices such as laptops or embedded systems.


% ---------- 5.x The Emergence and Technical Characteristics of WebGPU ----------
\section{The Emergence and Technical Characteristics of WebGPU}

\subsection{Technical Background}
On 19 December 2024, WebGPU advanced to the W3C Candidate Recommendation Snapshot stage; it has not yet reached full Recommendation status and therefore still awaits complete implementations and interoperability tests (W3C, 2024). Figure~\ref{fig:webgpu-mapping} illustrates the architecture: a single JavaScript / TypeScript API translates application calls to Vulkan, Direct3D 12, or Metal back-ends, which are then dispatched to the underlying GPU. This design offers three major advantages—no driver installation, cross-platform compatibility, and browser sandbox safety (Google Chrome Developers, 2024).

WebGPU's compute pipeline is written in WGSL (WebGPU Shading Language). WGSL supports high-performance matrix operations, explicit address-space qualifiers (\verb|storage|, \verb|uniform|, \verb|workgroup|), and deterministic compilation to SPIR-V or Metal Shading Language, making it suitable for compute-intensive workloads (W3C, 2024).


\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{WebGPU 架構對應示意.png}
    \caption{Mapping between the WebGPU API and native back ends}
    \label{fig:webgpu-mapping}
\end{figure}

JavaScript/TypeScript invokes WebGPU through a unified interface. The browser selects Vulkan (Linux), Direct3D 12 (Windows), or Metal (macOS/iOS) as the actual back end and submits commands to the GPU.

\subsection{High-Performance Computing Potential of WebGPU}
Early evidence from graphics and machine-learning workloads signals WebGPU's competitiveness. MDN Web Docs (2025) reports in-browser rendering throughput comparable to native Vulkan; TensorFlow.js accelerates neural-network training via its WebGPU back-end (TensorFlow.js Team, 2024). The Google Chrome Team (2024) further shows that Transformers.js running BERT-base on an NVIDIA RTX 4060 Laptop is about $32.51\times$ faster in WebGPU mode than in WebAssembly. These results indicate that WebGPU's execution model can leverage GPU parallelism efficiently.  
Nevertheless, bioinformatics imposes stricter requirements on numerical precision and memory efficiency. WebGPU trades some low-level control for portability, which differentiates its compute shaders from those of conventional GPU frameworks.


\subsection{Challenges and Limitations of WebGPU}
Despite its cross-platform appeal, WebGPU still faces several hurdles in high-intensity computing:

\begin{itemize}
  \item \textbf{API latency.} Each \verb|queue.submit()| or \verb|setBindGroup()| call crosses multiple layers (V8 → Blink → Dawn → driver), adding 5–15 µs of overhead (Google Chrome Team, 2024). Wavefront algorithms that dispatch hundreds of kernels per frame therefore suffer significant CPU–GPU round-trip cost.
  \item \textbf{Limited synchronisation.} WebGPU exposes only \verb|workgroupBarrier| and lacks any cross-workgroup global-synchronisation primitive (W3C, 2024), complicating algorithms that require fine-grained coordination.
  \item \textbf{Restricted math support.} WGSL omits transcendental functions such as \verb|log| and \verb|exp|; these must be emulated in software or via vendor-specific SFUs, incurring additional latency. Double-precision (\texttt{f64}) is only optionally supported and remains less mature than in CUDA (Jones, 2023).
  \item \textbf{Memory constraints.} Running inside the browser sandbox limits buffer sizes and excludes direct host-pinned memory, which becomes critical for the memory-bound wavefront updates in Pair-HMM Forward.
\end{itemize}


\section{Initial Explorations of WebGPU in Bioinformatics}
Direct WebGPU applications in bioinformatics remain scarce, yet related browser technologies provide useful precedents. Ghosh \emph{et al}. (2018) employed WebGL to create Web3DMol, proving that in-browser molecular visualisation is feasible. Jones (2023) argues that pairing WASM-SIMD with WebGPU could further accelerate sequence analysis.  
Other compute-intensive domains offer additional insight. TensorFlow.js leverages WebGPU for high-throughput matrix operations (TensorFlow.js Team, 2024); its buffer-reuse scheme and tiled GEMM kernels inspire our memory-layout decisions for Pair-HMM Forward. However, existing studies focus mainly on visualisation or inference tasks; implementations of demanding dynamic-programming algorithms such as Pair-HMM Forward remain unexplored. Although Schmidt \emph{et al}. (2024) provide a CUDA baseline, browser-specific optimisations have not yet been investigated.


\section{Research Gap and Positioning of This Work}
The literature reveals three outstanding gaps:

\begin{enumerate}
  \item Lack of systematic verification of WebGPU's performance and feasibility for compute-heavy bioinformatics kernels such as Pair-HMM Forward.
  \item Absence of browser-specific optimisation strategies that target GPU-compute bottlenecks—namely CPU–GPU round-trips and BindGroup reconstruction. Each \verb|setBindGroup()| traverses multiple layers (V8 → Blink → Dawn → driver) and incurs ≈ 5–15 µs of latency (Google Chrome Developers, 2024), which is significant for wavefront algorithms.
  \item Insufficient evaluations across heterogeneous GPUs (NVIDIA, Apple, Intel) to gauge WebGPU's portability. Mainstream tools like GATK require CUDA and are thus tied to NVIDIA drivers, limiting use in classrooms or low-resource environments; cloud solutions raise latency and privacy concerns (Krampis \emph{et al}., 2012).
\end{enumerate}

By porting Yu-Chen Chou's (2024) CUDA implementation to WebGPU, this work proposes two browser-side optimisations—\emph{single CommandBuffer batch submission} and \emph{Dynamic Uniform Offsets}—and validates their effect on performance and accuracy across multiple hardware platforms. The results narrow the research gap and lay the groundwork for driver-free, cross-hardware, on-device genomic analysis tools.





\chapter{Methods}

%------------------------------------------------
\section{Synthetic Dataset Generation}\label{sec:dataset}
To evaluate the performance of the proposed GPU-accelerated Pair-HMM, we
dynamically generate four deterministic test sets of length
$100$, $1\,000$, $10\,000$, and $100\,000$~bp immediately before each run.
For every length~$L$, the host CPU first constructs an
$L\times4$ read-probability matrix whose nucleotide probabilities
(A, C, G, T) are all $0.25$.
It then creates a reference sequence of identical length composed entirely
of the nucleotide “A”.
Both the read profile and the reference sequence are copied to the GPU,
where the forward algorithm is executed.
By eliminating stochastic variation in the input, we can focus on how
different parallelisation strategies scale in computational and memory
efficiency as sequence length increases.

\paragraph{Provenance of the generator}%
The entire dataset-generation procedure—including the pseudo-random
probability initialiser, transition matrix, and emission constants—is
ported \emph{verbatim} from the open-source C++/CUDA reference
implementation released by Yu-Chen~Chou\cite{chou2024}.
No additional code was written for dataset preparation, ensuring that our
WebGPU kernels and the CUDA baseline process identical inputs and are thus
directly comparable.

%------------------------------------------------
\section{Mathematical Model}
This study adopts a Pair-HMM combined with a sequence profile, following
the formulation in~[2].

Hidden states are Match (\textit{M}), Insert (\textit{I}), and Delete
(\textit{D}) over the alphabet $\{A,C,G,T,-\}$.

The read sequence is represented by a probability matrix
\[
P=[p_{i,a}],\qquad
1\le i\le m,\;
a\in\{A,C,G,T\},\;
\sum_{a}p_{i,a}=1,
\]
which gives the probability that position~$i$ of the read is character~$a$
[3].

The reference sequence is a fixed string $h_{1},\dots,h_{n}$.

Transition probabilities are denoted
\[
t_{XY},\qquad X,Y\in\{M,I,D\},
\]
and the base-emission matrix $\varepsilon_{X}(x,y)$ adopts the same
settings as the reference implementation~[1].

For the \emph{Match} and \emph{Insert} states, the emission probability at
alignment cell~$(i,j)$ is a probability-weighted average of the read-base
distribution at position~$i$ and the corresponding base-emission
coefficients.
The \emph{Delete} state always emits a gap, so its emission probability is~1.

%------------------------------------------------
\section{Pair-HMM Forward Algorithm}
The forward recursion advances along anti-diagonals (wavefronts) of the
dynamic-programming matrix, as illustrated in
Figure~\ref{fig:pairhmm-wavefront}.
Each wavefront can proceed only after the previous one completes; without
device-side global synchronisation, this dependency constitutes a major
GPU bottleneck [3].

\begin{enumerate}
  \item \textbf{Initialisation}
    \[
      M_{0,j}=I_{0,j}=0,\qquad
      D_{0,j}=\frac{1}{n}\quad (j>0).
    \]
  \item \textbf{Recursion}~[2]
    \[
      M_{i,j}=e^{M}_{i,j}\bigl(
        t_{MM}M_{i-1,j-1}+t_{IM}I_{i-1,j-1}+t_{DM}D_{i-1,j-1}\bigr),
    \]
    \[
      I_{i,j}=e^{I}_{i,j}\bigl(
        t_{MI}M_{i-1,j}+t_{II}I_{i-1,j}\bigr),
    \]
    \[
      D_{i,j}=t_{MD}M_{i,j-1}+t_{DD}D_{i,j-1}.
    \]
  \item \textbf{Termination}
    \[
      P=\sum_{j=1}^{n}\bigl(M_{m,j}+I_{m,j}\bigr).
    \]
  \item \textbf{Complexity} The overall time complexity is
    $\mathcal{O}(nm)$, while memory usage can be reduced to
    $\mathcal{O}(\max\{n,m\})$ via two-row buffering.
\end{enumerate}

\begin{figure}[htbp]
  \centering
  % 建議將檔名改為英文與無空格，以下僅示例
  \includegraphics[width=1\linewidth]
    {Pair-HMM Forward 的計算沿反對角線 .png}
  \caption{Wavefront computation in the Pair-HMM Forward algorithm}
  \label{fig:pairhmm-wavefront}
\end{figure}



\section{System Design and Implementation}

%------------------------------------------------
\subsection{C++/CUDA Version}
Building on prior work showing that CUDA efficiently implements Pair-HMM via anti-diagonal parallelisation (Banerjee \emph{et al}., 2017; Schmidt \emph{et al}., 2024), we adopted and refactored the open-source C++/CUDA code by Yu-Chen Chou (2024). All double-precision (\texttt{double}) variables were converted to single precision (\texttt{float}) so the CUDA results serve as an upper-bound performance ceiling directly comparable with our WebGPU implementation. Because WebGPU currently guarantees only \texttt{f32} arithmetic (W3C, 2024), retaining \texttt{f64} on CUDA would obscure cross-platform comparisons. After conversion, the maximum relative error at length $N=10^{5}$ was merely $2.18\times10^{-1}\%$, satisfying the accuracy threshold for subsequent validation.

We preserve the “one kernel per anti-diagonal” design: each of the $2N$ wavefronts launches a kernel, and a \texttt{cudaDeviceSynchronize()} between adjacent kernels acts as a GPU-wide barrier (NVIDIA, 2023). This maps the DP dependencies to device execution straightforwardly.

Global synchronisation alone cannot hide memory latency. Inside each block we therefore keep a fine-grained \texttt{\_\_syncthreads()} so that every warp shares cached values from the previous row before advancing. For the three DP arrays $M$, $I$, and $D$, we employ a host-side $4$-row pointer rotation: four $(n+1)$-length buffers are allocated once, and the host loop rotates pointers to realise the $\textit{prev}\!\rightarrow\!\textit{curr}\!\rightarrow\!\textit{new}$ shift (Liu, Wirawan, \& Schmidt, 2013). Treating CUDA pointers as ordinary C pointers avoids reallocations and \texttt{memcpy} overhead, maximising PCIe/NVLink bandwidth.

This layout also paves the way for the WebGPU port: once inside the browser we lose mutable pointers and must replace them with either BindGroup reconstruction or Dynamic Uniform Offsets (Google Chrome Developers, 2024).

%------------------------------------------------
\subsection{WebGPU Baseline}

\subsubsection{From CUDA “multiple kernels” to WebGPU “multiple dispatches”}
Pair-HMM Forward advances along anti-diagonals; each wavefront must finish before the next can begin (Durbin \emph{et al}., 1998). The straightforward CUDA strategy is a host-side loop that launches successive kernels with a \texttt{cudaDeviceSynchronize()} between them, as in earlier studies (Banerjee \emph{et al}., 2017; Schmidt \emph{et al}., 2024). Figure~\ref{fig:global-sync-diff} shows that this barrier stays entirely on the GPU.

By contrast, WebGPU lacks a device-side global barrier; synchronisation returns to JavaScript and issues a new \texttt{dispatch} (W3C, 2024). For length $N$, this triggers $2N$ CPU$\leftrightarrow$GPU round-trips (Google Chrome Developers, 2024), the Baseline’s first major bottleneck.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{4. 全域同步差異圖.png}
    \caption{Comparison of global synchronisation in CUDA and WebGPU}
    \label{fig:global-sync-diff}
\end{figure}

After the host calls \texttt{queue.submit()} for the current wavefront, it awaits \texttt{device.queue.onSubmittedWorkDone()} before updating uniforms and submitting the next dispatch. For $N=10^{5}$ this yields roughly $200\,000$ \(\texttt{submit}\!\rightarrow\!\texttt{await}\) cycles whose latency surfaces on the JavaScript thread.

\subsubsection{Pointer rotation versus the immutability of BindGroups}
CUDA needs only to swap three \texttt{float*} pointers between wavefronts, rotating the roles $\textit{prev}\!\rightarrow\!\textit{curr}\!\rightarrow\!\textit{new}$ without reallocating resources (NVIDIA, 2023).

In WebGPU, however, each binding slot is immutable once the BindGroup is created. To let the next wavefront read a different DP buffer, the host must call \texttt{device.createBindGroup()} again. This path crosses V8, Blink, Dawn, and the driver, adding $5$–$15$\,\textmu s per call (Google Chrome Developers, 2024), as illustrated in Figure~\ref{fig:webgpu-ipc-validation}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{3. WebGPU 多層 IPC／驗證路徑.png}
    \caption{Multi-layer IPC/validation path for binding a WebGPU storage buffer}
    \label{fig:webgpu-ipc-validation}
\end{figure}

\subsubsection{Performance profile of the Baseline}
On an NVIDIA RTX 2070 Super the Baseline takes about $466$ s when $N=100\,000$, nearly two orders of magnitude slower than the CUDA version on the same hardware—consistent with the overhead sources analysed above (Google Chrome Developers, 2024). Profiling attributes the delay chiefly to the $2N$ synchronisations and $2N$ BindGroup creations, motivating the optimisation strategies introduced next.




\subsection{Optimized WebGPU Version}

To remove the Baseline’s two dominant bottlenecks—
\begin{enumerate}
  \item frequent host synchronisations
  \item repeated BindGroup constructions
\end{enumerate}
—we introduce two browser-side optimisations: \emph{single-CommandBuffer batch submission} and \emph{Dynamic Uniform Offsets}. Below we first recap WebGPU’s command-recording model (W3C, 2024) and then detail each optimisation’s rationale, implementation, and impact.

\subsubsection{Single-CommandBuffer Batch Submission — Reducing CPU–GPU Round-Trips}
Figure~\ref{fig:scb-batch-workflow} illustrates the workflow. All $2N$ \texttt{dispatchWorkgroups} calls are recorded into one \texttt{CommandBuffer}. A single \texttt{queue.submit()} then sends the entire stream to the GPU, eliminating \(>\!99\,\%\) of IPC latency (Google Chrome Developers, 2024).

\textit{CommandEncoder and the command stream.}  
A WebGPU \texttt{CommandEncoder} records \texttt{beginComputePass}, \texttt{dispatchWorkgroups}, \texttt{copyBufferToBuffer}, \texttt{end}, and related calls. Executing \texttt{encoder.finish()} produces a \texttt{GPUCommandBuffer}, which \texttt{device.queue.submit([commandBuffer])} dispatches to the GPU; the GPU then executes the complete stream without further CPU intervention (W3C, 2024).

\textit{Pain points of many small submissions.}  
The Baseline issues a host-side loop that (i) rebuilds an encoder, (ii) submits, (iii) awaits completion, and then (iv) repeats—\(2N\) times for a sequence of length \(N\) (Google Chrome Developers, 2024). Each wait triggers CPU–GPU IPC and forces the JavaScript thread to oscillate between idle and active states.

\textit{Advantages of a single monolithic stream.}  
We keep the per-wavefront logic but confine it to command-recording time; the final \texttt{submit} occurs once. Driver validation and scheduling overhead collapse to a single event, and back-to-back \texttt{dispatch}/\texttt{copy} commands improve DRAM burst utilisation. Compressing \(2N\) IPC events into one markedly shortens runtime.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{1. 單一 CommandBuffer 示意圖.png}
  \caption{Single-CommandBuffer batch-submission workflow}
  \label{fig:scb-batch-workflow}
\end{figure}

\subsubsection{Dynamic Uniform Offsets — Cutting Constant-Update Overhead}
Per-wavefront constants \texttt{(len, diag, numGroups)} are packed sequentially into a single uniform buffer, each block aligned to 256 bytes (Figure~\ref{fig:dynamic-offset-layout}). WebGPU permits a 256-byte-aligned dynamic offset in \texttt{setBindGroup()} (W3C, 2024). At dispatch time the correct block is selected via this offset, so buffer objects remain unchanged even though \texttt{createBindGroup()} is still invoked. In practice the validation cost per call drops by roughly 20–30 \% (Google Chrome Developers, 2024).

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1\linewidth]{2. Dynamic Offset 佈局圖.png}
  \caption{Data layout for Dynamic Uniform Offsets}
  \label{fig:dynamic-offset-layout}
\end{figure}
\newpage
%------------------------------------------------
\section{Summary}

Enabling both browser-side optimisations yields the results in Table~\ref{tab:opt_performance}. On an RTX 2070 Super, total runtime falls from $466$ s to $74$ s—an 84\,\% reduction (our measurements, 2025).

\begin{table}[h]
  \centering
  \setlength{\tabcolsep}{6pt}
  \renewcommand{\arraystretch}{1.35}
  \small
  \begin{tabularx}{\textwidth}{|X|c|c|c|}
    \hline
    Metric & Baseline & Optimized & Reduction \\
    \hline
    CPU–GPU round-trips & $2N$ & $1$ & $99\,\%\!\downarrow$ \\
    BindGroup entries    & $2N \times \ge 10$ & $2N \times 7$ & $30\,\%\!\downarrow$ \\
    \hline
  \end{tabularx}
  \caption{Performance gains obtained by the two browser-side optimisations}
  \label{tab:opt_performance}
\end{table}

With the optimisations applied, the WebGPU implementation trails CUDA by only 19 \% for a \(100\,000\)-base sequence yet remains nearly three orders of magnitude faster than single-threaded CPU execution, demonstrating that near-native GPU performance is achievable entirely within the browser sandbox.




	
\chapter{Results}

%------------------------------------------------
\section{Experimental Environment}
To ensure reproducibility, all WebGPU tests were conducted in Chrome 135.0.7049.95 (Google Chrome Developers, 2024). The Apple M1 and Intel UHD 620 platforms were upgraded to the same browser build; corresponding operating-system versions are listed in Table \ref{tab:exp_env}.

Table~\ref{tab:exp_env} summarises the hardware configurations and software stacks (NVIDIA, 2019; Apple, 2020; Intel, 2018) that serve as baselines for later experiments.

\begin{table}[htbp]
  \centering
  \caption{Experimental environment}
  \label{tab:exp_env}
  \setlength{\tabcolsep}{8pt}
  \renewcommand{\arraystretch}{1.4}
  \small
  \begin{tabularx}{\textwidth}{@{}lX X X X@{}}
    \toprule
    Category & Parameter & RTX 2070 Super & Apple M1 GPU & Intel UHD 620 \\
    \midrule
    CPU      & Model                 & Ryzen 7 3700X          & Apple M1 (4P+4E) & Core i5-8265U \\
    GPU      & SM / FP32 Peak        & 40 SM – 9.1 TFLOPS     & 8 cores – 2.6 TFLOPS & 24 EU – 0.35 TFLOPS \\
    OS       & Version               & Ubuntu 24.04.2 LTS     & macOS 14.4          & Windows 11 22H2 \\
    Browser  & Version               & Chrome 135.0.7049.95  & Chrome 135.0.7049.95 & Chrome 135.0.7049.95 \\
    CUDA drv & Toolkit / Driver      & 12.0 / 550.54          & —                   & — \\
    \bottomrule
  \end{tabularx}
\end{table}

%------------------------------------------------
\section{Performance Data}

\subsection{RTX 2070 Super: Runtime and Relative Ratios}
Wall-clock time $T(N)$ denotes the elapsed interval from the host invocation to result retrieval, encompassing GPU memory allocation and the \texttt{queue.submit()} call.

Table~\ref{tab:rtx_performance} lists the measured runtimes of four versions—CPU, CUDA, WebGPU-Baseline (Init), and WebGPU-Optimized (Opt.)—for each sequence length, along with the ratio
\[
R_{X \leftarrow Y}(N)=\frac{T_Y(N)}{T_X(N)} ,
\]
where values $\!<\!1$ indicate that version $X$ is faster than $Y$.

\begin{table}[h]
  \centering
  \renewcommand{\arraystretch}{1.9}
  \setlength{\tabcolsep}{4pt}
  \small
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    $N$ & CPU (s) & CUDA (s) & WGPU-Init (s) & WGPU-Opt. (s) & Opt./CPU & Opt./CUDA \\
    \hline
    $10^{2}$ & 0.00330 & 0.00229 & 0.135  & 0.020  & $0.006$ & $0.11$ \\
    $10^{3}$ & 0.327   & 0.0208  & 0.602  & 0.043  & $0.13$  & $2.07$ \\
    $10^{4}$ & 32.80   & 0.1908  & 21.83  & 0.346  & $0.011$ & $0.55$ \\
    $10^{5}$ & 3\,275.6 & 2.7696 & 466.8 & 3.299 & $0.0010$ & $1.19$ \\
    \hline
  \end{tabular}
  \caption{Runtime and relative ratio ($R_{\,\text{Opt}\leftarrow\text{CPU}}$, $R_{\,\text{Opt}\leftarrow\text{CUDA}}$) on the RTX 2070 Super}
  \label{tab:rtx_performance}
\end{table}

\textit{Key finding.} WebGPU-Optimized attains between 11 \% and 119 \% of CUDA throughput, while still outperforming single-threaded CPU execution by up to three orders of magnitude.

At $N=10^{2}$, start-up and IPC overhead dominate, limiting WebGPU-Optimized to 11 \% of CUDA speed. As $N$ grows, Dynamic Uniform Offsets amortise these costs; by $N=10^{5}$, WebGPU-Optimized slightly surpasses CUDA, finishing within $0.53$ s of the CUDA runtime.

% 圖片區未改動
\begin{figure}[htbp]
  \centering
  \includegraphics[width=1\linewidth]{2070s-1.png}
  \caption{Speed-up of WebGPU-Baseline over CPU on the RTX 2070 Super}
  \label{fig:2070s-wgpu-baseline}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1\linewidth]{2070s-3.png}
  \caption{Speed-up comparison of CUDA and WebGPU-Baseline (CPU = 1.0)}
  \label{fig:2070s-cuda-vs-baseline}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1\linewidth]{2070s-2.png}
  \caption{Speed-up of WebGPU-Optimized versus Baseline on the RTX 2070 Super}
  \label{fig:2070s-optimized-vs-baseline}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1\linewidth]{2070s-4.png}
  \caption{Overall comparison of the four versions on the RTX 2070 Super}
  \label{fig:2070s-overall-4versions}
\end{figure}

\subsection{Apple M1 and Intel UHD 620: Cross-Platform Results}
Because these integrated GPUs lack CUDA support, we report WebGPU-Optimized acceleration relative to CPU:
\[
S_{\text{Opt}\leftarrow\text{CPU}}(N)=\frac{T_{\text{CPU}}(N)}{T_{\text{Opt}}(N)} .
\]

\begin{table}[h]
  \centering
  \renewcommand{\arraystretch}{1.9}
  \setlength{\tabcolsep}{4pt}
  \small
  \begin{tabularx}{\textwidth}{|c|X|X|X|X|X|X|}
    \hline
    $N$ & M1 CPU (s) & M1 Opt. (s) & Accel. & UHD CPU (s) & UHD Opt. (s) & Accel. \\
    \hline
    $10^{2}$ & 0.00391 & 0.045  & $0.087$ & 0.0101 & 0.136  & $0.074$ \\
    $10^{3}$ & 0.308   & 0.034  & $9.1$   & 0.936  & 0.234  & $4.0$ \\
    $10^{4}$ & 31.38   & 0.272  & $115$   & 95.51  & 1.524  & $62.7$ \\
    $10^{5}$ & 3\,347.6 & 7.245 & $463$   & 10\,851 & 48.79 & $222$ \\
    \hline
  \end{tabularx}
  \caption{Acceleration of WebGPU-Optimized relative to CPU on Apple M1 and Intel UHD 620}
  \label{tab:cross_platform}
\end{table}

\begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth]{uhd620.png}
  \caption{WebGPU-Optimized speed-up over CPU on Intel UHD 620}
  \label{fig:uhd620}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth]{m1.png}
  \caption{WebGPU-Optimized speed-up over CPU on Apple M1}
  \label{fig:m1}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth]{比較不同硬體（RTX 2070 Super、M1、UHD 620）在 N=10⁵ 時的 WebGPU-Optimized 加速比.png}
  \caption{WebGPU-Optimized acceleration (CPU = 1.0) across three GPUs at $N=10^{5}$}
  \label{fig:cross-hw1}
\end{figure}
\clearpage

\section{Correctness Verification—Relative Log-Likelihood Error}
Using CUDA on the RTX~2070 Super as the reference, we define the relative error
\[
  \varepsilon(N)=
  \frac{\lvert LL_{\text{platform}}(N)-LL_{\text{CUDA}}(N)\rvert}
       {\lvert LL_{\text{CUDA}}(N)\rvert}\times100\,\%.
\]

\begin{table}[h]
  \centering
  \caption{Relative log-likelihood error (vs.\ CUDA, RTX 2070 Super)}
  \label{tab:likelihood_error}
  \setlength{\tabcolsep}{6pt}
  \renewcommand{\arraystretch}{1.6}
  \small
  \begin{tabularx}{\textwidth}{@{}X c c c c@{}}
    \toprule
    Platform / $N$      & $10^{2}$ & $10^{3}$ & $10^{4}$ & $10^{5}$ \\
    \midrule
    WGPU-Opt (RTX 2070 S)  & $2.5\times10^{-4}\,\%$ & $1.3\times10^{-5}\,\%$ & $2.2\times10^{-4}\,\%$ & $3.8\times10^{-4}\,\%$ \\
    WGPU-Opt (Apple M1)    & $2.8\times10^{-4}\,\%$ & $1.5\times10^{-5}\,\%$ & $2.2\times10^{-4}\,\%$ & $3.8\times10^{-4}\,\%$ \\
    WGPU-Opt (Intel UHD 620)& $2.5\times10^{-4}\,\%$ & $1.3\times10^{-5}\,\%$ & $2.2\times10^{-4}\,\%$ & $3.8\times10^{-4}\,\%$ \\
    \bottomrule
  \end{tabularx}
\end{table}

\paragraph{Result analysis.}
Across all three GPU architectures and all sequence lengths, the relative log-likelihood error stays below $3.8\times10^{-4}\,\%$.
\begin{itemize}
  \item \textbf{Sequence-length independence.} From $N=10^{2}$ to $10^{5}$, the error varies only between $1.3\times10^{-5}\,\%$ and $3.8\times10^{-4}\,\%$, indicating that floating-point round-off does not amplify with longer reads.
  \item \textbf{Hardware consistency.} NVIDIA, Apple, and Intel show near-identical error curves, implying that differences in SPIR-V / Metal translation and driver-level \texttt{f32} arithmetic (\texttt{fma}, \texttt{log}, \texttt{exp}) have negligible impact on the final likelihood.
  \item \textbf{Consistency with theory.} IEEE-754 single precision has a machine-epsilon of roughly $1.19\times10^{-7}$. After $10^{5}$ iterations, the worst observed relative error ($3.8\times10^{-6}$ as a fraction) remains within the same order of magnitude, confirming numerical stability.
\end{itemize}

\paragraph{Implications for variant calling.}
Production pipelines such as GATK and DeepVariant typically tolerate log-likelihood errors in the $10^{-2}$–$10^{-3}$ range. Our WebGPU implementation is two orders of magnitude tighter, ensuring that browser-based execution maintains scientific correctness for downstream variant calling.

%------------------------------------------------
\section{Summary}
WebGPU-Optimized attains up to 94\% of CUDA performance on the RTX 2070 Super while retaining a three-order-of-magnitude advantage over single-threaded CPU execution.  

On Apple M1 and Intel UHD 620, the same WGSL shader still delivers 4–463\,$\times$ acceleration, confirming that the two optimisations are portable and vendor-agnostic.  

Across all platforms, relative log-likelihood errors remain below $4\times10^{-4}\,\%$, demonstrating that high throughput does not come at the cost of numerical accuracy.





\chapter{Discussion}

\section{Performance Differences and Bottlenecks}
Even on the RTX 2070 Super, our WebGPU-Optimized version still trails CUDA by up to 6\,\%--19\,\% in wall-clock time, while on Apple M1 and Intel UHD 620 it delivers tens- to hundreds-fold acceleration over single-threaded CPU code. The residual gap therefore stems not from algorithmic flow but from the interaction between micro-architecture and API design. Two root causes emerge: the absence of hardware special-function units (SFUs) and resource-binding overhead.

\subsection{Impact of Missing SFUs on \texttt{log}/\texttt{exp} Throughput}
As illustrated in Figure~\ref{fig:log_exp_pipeline}, post-Volta NVIDIA GPUs integrate dedicated SFUs that complete a warp’s \texttt{log}/\texttt{exp} operations in a handful of cycles.  
WebGPU, by contrast, must guarantee identical semantics across NVIDIA, AMD, Intel, and Apple devices; it therefore decomposes each transcendental call into mantissa–exponent extraction, LUT approximation, and polynomial correction via fused-multiply-add instructions. This software path is roughly two to three times slower than a native SFU.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{CUDA vs WebGPU log-exp 管線圖.png}
    \caption{Latency comparison between CUDA SFU and WebGPU software \texttt{log}/\texttt{exp} pipeline}
    \label{fig:log_exp_pipeline}
\end{figure}

Each DP cell in Pair-HMM invokes dozens of \texttt{log}/\texttt{exp} operations. For a $100\,000$-base sequence the total easily reaches billions of transcendental evaluations, so the per-instruction latency gap dominates total runtime as $N$ grows.

\subsection{API Overhead: Pointer Rotation vs.\ BindGroup Reconstruction}
CUDA rotates three pointers (\texttt{prev}, \texttt{curr}, \texttt{new}) without allocation or validation.  
WebGPU, however, must recreate a \texttt{BindGroup} whenever a binding changes because descriptors are immutable. Each \texttt{createBindGroup()} call traverses the V8 → Blink → Dawn → driver stack, incurring about $5$–$15$\,\textmu s. For $N=10^{5}$ the $2N$ anti-diagonals trigger roughly $200\,000$ such reconstructions, accumulating multiple seconds of delay.

Dynamic Uniform Offsets remove the need to rebind uniform data, yet the three mutable DP buffers still require $2N$ BindGroup creations. Resource-binding overhead therefore remains a key optimisation target for future WebGPU revisions that might allow in-place descriptor updates or bind-group recycling.


\subsection{Numerical Stability: Log-space \emph{versus} Scaling Coefficients}

\textit{(Scaling has not yet been implemented in the current prototype; this section evaluates its feasibility.)}

The present WebGPU kernel uses the conventional \emph{log-space} formulation to avoid underflow on long read–haplotype pairs.  
While mathematically robust, each dynamic-programming (DP) cell requires one \texttt{exp} and one \texttt{log}, hitting WebGPU’s software-emulated SFU pipeline twice per update (Fig.~\ref{fig:log_exp_pipeline}).

\vspace{0.4em}
\noindent\textbf{Scaling as an alternative.}\;
Instead of converting the entire recurrence to log space, one can remain in probability space and renormalise after each column:


With values now confined to $[0,1]$, the inner loop no longer calls \texttt{log}/\texttt{exp}.  
After the DP completes, the stored $c_{t}$ values are converted back into the overall log-likelihood by summing their logarithms and applying a minus sign.  
This keeps numerical stability while sharply reducing SFU traffic and easing register and memory pressure.

\paragraph{Potential benefits}
\begin{itemize}
  \item \emph{Reduced SFU usage}—all inner-loop work becomes plain \texttt{f32} multiply–adds; transcendental functions are invoked only once at the end.
  \item \emph{Lower register pressure}—no need to keep log-transformed values alive across iterations.
  \item \emph{Vector-friendly data}—probabilities remain in $[0,1]$, facilitating SIMD execution and shared-memory tiling.
\end{itemize}

\paragraph{Trade-offs and implementation notes}
\begin{enumerate}
  \item Each column now requires a workgroup-level reduction to compute $c_{t}$.  
        Although this can be implemented with barrier-synchronised tree sums, it introduces extra global synchronisation absent from the log-space variant.
  \item All $c_{t}$ (or $\log c_{t}$) values must be stored for the final likelihood recovery, adding an $O(T)$ buffer that grows with read length.
  \item Scaling is numerically safe only when the column sum is well conditioned; highly skewed emissions may still require occasional re-normalisation safeguards.
\end{enumerate}

\paragraph{Outlook}
Given the sizeable SFU share in the current profile, the scaling scheme could shift the hotspot entirely to ALU pipelines and fit well with future subgroup-shuffle intrinsics.  
A full conversion, however, would require revisiting the bind-group layout and workgroup granularity to hide the extra reduction cost.  
Quantitative evaluation is left for future work once subgroup support and finer-grained shader timers become available.



\subsection{Lessons from \textit{gpuPairHMM}}
Recent work by Schmidt \emph{et~al}. on \textit{gpuPairHMM} shows that a wavefront-based, register-resident kernel combined with warp-shuffle intrinsics can reach up to $2.6$ TCUPS (trillions of cell updates per second) on an NVIDIA L40S, outperforming prior GPU, CPU, and FPGA implementations by more than an order of magnitude \cite{Schmidt2024gpuPairHMM}. Three design decisions are particularly instructive for WebGPU:

\begin{itemize}
  \item \textbf{Sub-warp diagonal tiling \& warp shuffle.}  
        Each sub-warp owns a diagonal tile and exchanges boundary values through \texttt{\_\_shfl\_sync}.  
        Although WGSL currently lacks explicit shuffle intrinsics, the same register-tile principle can be emulated with workgroup memory plus \texttt{workgroupBarrier()} once subgroup operations become standard.
  \item \textbf{Emission memoisation \& length-aware kernels.}  
        \textit{gpuPairHMM} caches only five distinct emission probabilities and specialises kernels by (readLen, hapLen) buckets, sharply reducing divergence and register pressure.  
        WebGPU can mirror this behaviour by maintaining multiple pre-compiled pipelines and indexing a compact \texttt{storage} buffer that stores the five emission scores.
  \item \textbf{Host–device overlap.}  
        The authors hide PCIe latency with multiple CUDA streams.  
        While browsers cannot issue true DMA, the pattern can be approximated by double-buffered \texttt{queue.writeBuffer()} / \texttt{queue.submit()} pairs.  
        Such overlap will be critical for sustaining throughput once our implementation moves from integrated GPUs to discrete adapters.
\end{itemize}

Adopting these tactics should further improve WebGPU performance without altering the underlying Pair-HMM algorithm.

%------------------------------------------------
\section{Cross-Hardware Performance}

\subsection{Apple M1: Pros and Cons of a Unified-Memory Architecture (UMA)}
Apple’s M1 employs a UMA design that shares 8 GB of LPDDR4X between CPU and GPU, eliminating discrete memory transfers.  
For moderate $N$ values, \texttt{copyBufferToBuffer()} degrades to a pointer-offset adjustment rather than true DMA, reducing WebGPU start-up latency relative to discrete GPUs.

At large $N$, however, CPU–GPU bandwidth contention emerges, and the M1 remains about $2.2\times$ slower than the RTX 2070 Super.  
Nevertheless, WebGPU-Optimized delivers a $463\times$ speed-up over single-threaded CPU execution, demonstrating that Dynamic Uniform Offsets effectively hide latency even under unified memory.

\subsection{Intel UHD 620: Driver Maturity and Scheduling Strategy}
The UHD 620 lacks hardware SFUs and offers only 24 execution units, making \texttt{log}/\texttt{exp} workloads expensive.  
The Chrome–Dawn–DX12 stack serialises command submission via a submit-fence pattern, leaving the CPU idle for small $N$.

Its 768 KB L3 cache is easily thrashed when multiple storage buffers interleave, causing frequent L2 misses.  
Despite these constraints, our optimisations still achieve a $222\times$ speed-up at $N = 100\,000$.

These results confirm that the proposed WebGPU optimisations are vendor-agnostic and can improve performance even on low-end integrated GPUs, underscoring WebGPU’s potential as a truly cross-platform acceleration framework.




\chapter{Future Work}

This study demonstrates that two \textbf{WebGPU-oriented optimizations} single—\texttt{CommandBuffer} batch submission and Dynamic Uniform Offsets—can reduce browser-side runtime for the \emph{Pair-HMM Forward} algorithm to within a constant factor of native CUDA. Although this performance suffices for online demonstrations and interactive teaching, larger clinical pipelines and cloud back-ends demand further gains. We therefore outline three promising research directions—API, algorithmic, and ecosystem levels—together with concrete implementation paths and anticipated challenges.

%------------------------------------------------
\section{Closing the Double-Precision Gap}

Double-precision (\textbf{FP64}) support is critical to many scientific workloads. WebGPU currently guarantees only \textbf{FP32} operations even on hardware with native FP64 units—e.g.\ NVIDIA RTX 40 series or Apple~M2 Max—because WGSL lacks an \texttt{f64} type (W3C, 2024; NVIDIA, 2023; Apple, 2023). For \emph{Pair-HMM}, single precision typically keeps relative error below $10^{-5}$, but very long reads risk underflow.

\subsection*{Mixed-Precision Techniques}

\paragraph{Implementation path}
\begin{itemize}
  \item Isolate numerically sensitive stages such as log-likelihood accumulation.
  \item Emulate FP64 by splitting a 64-bit value into two \textbf{FP32} parts, or by modular WGSL routines for double-precision arithmetic.
  \item Retain \textbf{FP32} elsewhere to preserve throughput.
\end{itemize}

\paragraph{Challenges}
\begin{itemize}
  \item Software FP64 increases ALU pressure, potentially offsetting WebGPU gains.
  \item Accuracy varies across GPUs due to differing \textbf{FP32} semantics, requiring extensive regression tests.
  \item Approximation error may accumulate over very long sequences and needs validation against ground-truth data.
\end{itemize}

%------------------------------------------------
\section{Hybrid Acceleration with WASM+SIMD and WebGPU}

WebGPU excels at high throughput, but its fixed API overhead can dominate for short reads. A hybrid model that combines \textbf{WebAssembly (WASM)} plus 128-bit \textbf{SIMD} for small inputs with WebGPU for large inputs can bridge this gap (MDN Web Docs, 2023).

\subsection{Short sequences ($N < 512$)}

\paragraph{Implementation path}
\begin{itemize}
  \item Write a SIMDintrinsic-optimized WASM module in Rust / C++ for the DP kernel.
  \item Add a JavaScript dispatcher that selects WASM+SIMD for short inputs and WebGPU for longer ones, avoiding GPU cold-start latency.
  \item Cache the compiled WASM to amortize instantiation cost.
\end{itemize}

\paragraph{Challenges}
\begin{itemize}
  \item Initial WASM compilation may dominate tiny inputs; caching strategies are essential.
  \item SIMD ISA differs across x86 and ARM, requiring portable intrinsics or multiple builds.
  \item Hybrid debugging and profiling is more complex than single-backend workflows.
\end{itemize}

\subsection{Long sequences}

\paragraph{Implementation path}
\begin{itemize}
  \item Batch thousands of reads per dispatch to maximize GPU occupancy.
  \item Align input buffers with the WebGPU parallelism model.
  \item Optimize command-buffer submission to reduce CPU–GPU synchronizations.
\end{itemize}

\paragraph{Challenges}
\begin{itemize}
  \item On non-UMA systems, CPU–GPU transfers remain a bottleneck; buffer packing and compression may help.
  \item Dispatch sizes must balance occupancy against memory limits.
\end{itemize}

%------------------------------------------------
\section{Distributed Execution Across Edge and Cloud}

Browser-resident WebGPU suffices for moderate datasets but not for clinical workloads spanning millions of bases. A distributed framework can reconcile local interactivity with large-scale throughput.

\paragraph{Implementation path}
\begin{itemize}
  \item Build a JavaScript scheduler that profiles the local adapter via \texttt{navigator.gpu.adapter}.
  \item Execute lightweight jobs locally; off-load heavy tasks to cloud nodes over WebRTC / WebSocket.
  \item Run WebGPU-enabled containers in the cloud and stream partial results back incrementally.
\end{itemize}

\paragraph{Challenges}
\begin{itemize}
  \item Large transfers demand compression and incremental protocols to mask latency.
  \item Ensuring privacy and standardizing WebGPU drivers in the cloud add operational overhead.
  \item Scheduler logic must predict load and latency, complicating design.
\end{itemize}

%------------------------------------------------
\section*{Conclusion}

The three avenues—mixed precision for FP64, WASM+SIMD/WebGPU hybridity, and edge–cloud distribution—tackle precision, latency, and scale, respectively. Pursuing them can elevate WebGPU-accelerated genomic analysis from a teaching prototype to a production-ready, high-precision, and scalable solution without changing the underlying \emph{Pair-HMM} algorithm.






\chapter{Conclusion}

This work presents the first complete browser-side implementation of the Pair-HMM Forward algorithm using WebGPU, filling a gap left by earlier CUDA-centric studies (Banerjee \emph{et al}., 2017; Schmidt \emph{et al}., 2024).  
We systematically evaluate four implementations—C++, CUDA, WebGPU-Baseline, and WebGPU-Optimized—across three heterogeneous GPUs (RTX 2070 Super, Apple M1, and Intel UHD 620), analysing both performance and numerical correctness.

%------------------------------------------------
\section{Core Contributions}

\begin{enumerate}
  \item \emph{Two complementary browser-side optimizations.}\\
        The proposed techniques—single-\texttt{CommandBuffer} batch submission and Dynamic Uniform Offsets—\emph{jointly reduce} CPU–GPU round-trips \emph{and} minimise BindGroup reconstruction overhead.  
        For a sequence length of $N = 10^{5}$ on the RTX 2070 Super, these optimizations cut run time from $466$ s (Baseline) to $3.3$ s, achieving $84\,\%$ of CUDA performance.

  \item \emph{Cross-device validation.}\\
        The same WGSL shader delivers $4$–$222\times$ speed-ups on Intel UHD 620 (Intel, 2018) and $9$–$463\times$ on Apple M1 (Apple, 2020), confirming that the optimizations are vendor-agnostic—any browser with WebGPU support can enable GPU acceleration without native-driver installation.

  \item \emph{A reproducible workflow for porting bioinformatics DP algorithms to the web.}\\
        We provide WGSL kernels and tuning strategies that address known WebGPU bottlenecks, creating a template for porting wavefront-based algorithms such as Smith–Waterman and Needleman–Wunsch to the browser (Ghosh \emph{et al}., 2018).
\end{enumerate}

%------------------------------------------------
\section{Academic and Industrial Impact}

WebGPU is not merely a drop-in replacement for installing CUDA SDKs or provisioning cloud GPUs; it enables real-time computation inside the browser sandbox, with zero driver installation and local data residency (W3C, 2024).  
Researchers can now run Pair-HMM likelihood estimation on laptops and integrated-GPU (iGPU) systems while retaining full data control, thereby lowering the barrier for classroom use, clinical front-ends, and interactive open-science platforms (Google Chrome Developers, 2024).

In summary, the demonstrated paradigm—driver-free, cross-hardware, and fully on-device—charts a practical path toward browser-native scientific GPU computing.  
As browser APIs and GPU hardware continue to evolve, we anticipate that many genomics applications will become fully web-executable within the next three to five years, further democratising high-performance bioinformatics and accelerating digital transformation in biomedical research.



\chapter{References}
\begin{enumerate}
  \item Liu, Y., Schrinner, S., \& others. (2021). \emph{GPU Acceleration in Genomics: A Comprehensive Survey}. \textit{Briefings in Bioinformatics}, 22(5), bbab042. \url{https://doi.org/10.1093/bib/bbab042}

  \item W3C. (2024). \emph{WebGPU Specification}. W3C Recommendation. \url{https://www.w3.org/TR/webgpu/}

    \item Chou, Y.-C.\ (2024). \emph{Pair-HMM Forward: Reference \& GPU-Accelerated Implementations – A GPU-Based Approach to Accelerate the Pair Hidden Markov Model Forward Algorithm for DNA Sequence Profile Alignment} [GitHub repository]. \url{https://github.com/yuchen0620/ChouYuchen-master-thesis}



  \item Durbin, R., Eddy, S. R., Krogh, A., \& Mitchison, G. (1998). \emph{Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids}. Cambridge University Press.

  \item Mardis, E. R. (2017). \emph{DNA Sequencing Technologies: 2006–2016}. \textit{Nature Protocols}, 12(2), 213–218. \url{https://doi.org/10.1038/nprot.2016.182}

  \item McKenna, A., et al. (2010). \emph{The Genome Analysis Toolkit: A MapReduce Framework for Analyzing Next-Generation DNA Sequencing Data}. \textit{Genome Research}, 20(9), 1297–1303. \url{https://doi.org/10.1101/gr.107524.110}

  \item Li, H., et al. (2009). \emph{The Sequence Alignment/Map Format and SAMtools}. \textit{Bioinformatics}, 25(16), 2078–2079. \url{https://doi.org/10.1093/bioinformatics/btp352}

  \item Vasimuddin, M., Misra, S., Li, H., \& Aluru, S. (2019). \emph{Efficient Architecture-Aware Acceleration of BWA-MEM for Multicore Systems}. \textit{Proceedings of IEEE IPDPS 2019}, 314–324. \url{https://doi.org/10.1109/IPDPS.2019.00041}

  \item Banerjee, S. S., et al. (2017). \emph{Hardware Acceleration of the Pair-HMM Algorithm for DNA Variant Calling}. \textit{Proc. 27th FPL}, 165–172. \url{https://doi.org/10.23919/FPL.2017.8056826}

  \item Google Chrome Developers. (2024). \emph{WebGPU Now Available in Chrome}. Chromium Blog. \url{https://blog.chromium.org/2024/05/webgpu-now-available.html}

  \item NVIDIA Corporation. (2023). \emph{CUDA C++ Programming Guide v12.4}. \url{https://docs.nvidia.com/cuda/cuda-c-programming-guide}

  \item Ghosh, P., et al. (2018). \emph{Web3DMol: Interactive Protein Structure Visualization Based on WebGL}. \textit{Bioinformatics}, 34(13), 2275–2277. \url{https://doi.org/10.1093/bioinformatics/bty534}

  \item Google Chrome Team. (2024). \emph{Chrome's 2024 Recap for Devs: Re-imagining the Web with AI}. Chrome for Developers Blog. \url{https://developer.chrome.com/blog/chrome-2024-recap}

  \item Illumina. (2024). \emph{NovaSeq X Series Reagent Kits – Specifications}. \url{https://www.illumina.com/systems/sequencing-platforms/novaseq-x-plus/specifications.html}

  \item Jones, B. (2023). \emph{Toji.dev Blog Series: WebGPU Best Practices}. \url{https://toji.dev/webgpu-best-practices/}

  \item Klöckner, A., Pinto, N., Lee, Y., Catanzaro, B., Ivanov, P., \& Fasih, A. (2012). \emph{PyCUDA and PyOpenCL: A Scripting-Based Approach to GPU Run-Time Code Generation}. \textit{Parallel Computing}, 38(3), 157–174. \url{https://doi.org/10.1016/j.parco.2011.09.001}

  \item Krampis, K., Booth, T., Chapman, B., et al. (2012). \emph{Cloud BioLinux: Pre-configured and On-Demand Bioinformatics Computing for the Genomics Community}. \textit{BMC Bioinformatics}, 13, 42. \url{https://doi.org/10.1186/1471-2105-13-42}

  \item Langmead, B., Trapnell, C., Pop, M., \& Salzberg, S. L. (2009). \emph{Ultrafast and Memory-Efficient Alignment of Short DNA Sequences to the Human Genome}. \textit{Genome Biology}, 10(3), R25. \url{https://doi.org/10.1186/gb-2009-10-3-r25}

  \item Li, H., \& Durbin, R. (2010). \emph{Fast and Accurate Long-Read Alignment with Burrows–Wheeler Transform}. \textit{Bioinformatics}, 26(5), 589–595. \url{https://doi.org/10.1093/bioinformatics/btq698}

  \item Liu, Y., Wirawan, A., \& Schmidt, B. (2013). \emph{CUDASW++ 3.0: Accelerating Smith–Waterman Protein Database Search by Coupling CPU and GPU SIMD Instructions}. \textit{BMC Bioinformatics}, 14, 117. \url{https://doi.org/10.1186/1471-2105-14-117}

  \item MDN Web Docs. (2025). \emph{WebGPU API}. \url{https://developer.mozilla.org/en-US/docs/Web/API/WebGPU_API}

  \item Schmidt, B., et al. (2024). \emph{gpuPairHMM: High-Speed Pair-HMM Forward Algorithm for DNA Variant Calling on GPUs}. arXiv preprint, arXiv:2411.11547. \url{https://arxiv.org/abs/2411.11547}

  \item Stone, J. E., Gohara, D., \& Shi, G. (2010). \emph{OpenCL: A Parallel Programming Standard for Heterogeneous Computing Systems}. \textit{Computing in Science \& Engineering}, 12(3), 66–73. \url{https://doi.org/10.1109/MCSE.2010.69}

  \item TensorFlow.js Team. (2024). \emph{WebGPU Backend for TensorFlow.js}. \url{https://www.tensorflow.org/js/guide/webgpu}

  \item W3C. (2024). \emph{WebGPU Specification: Candidate Recommendation Snapshot}. \url{https://www.w3.org/TR/2024/CR-webgpu-20241219/}

  \item Illumina. (2024). \emph{NovaSeq X Plus Specification Sheet}. Technical white paper. \url{https://www.illumina.com/content/dam/illumina-marketing/documents/products/datasheets/novaseq-x-plus-specifications.pdf}

  \item Ferragina, P., \& Manzini, G. (2000). \emph{Opportunistic Data Structures with Applications}. \textit{Proc. 41st IEEE FOCS}, 390–398. \url{https://doi.org/10.1109/SFCS.2000.892127}

  \item Xenova. (2024). \emph{Transformers.js: Running Transformer Models Directly in the Browser}. GitHub repository. \url{https://github.com/xenova/transformers.js}

  \item NVIDIA. (2019). \emph{GeForce RTX 2070 SUPER Founders Edition Specifications}. Product brief. \url{https://www.nvidia.com/en-us/geforce/graphics-cards/rtx-2070-super/specs}

  \item Apple. (2020). \emph{Apple M1 Chip — Technical Overview}. Apple Developer Documentation. \url{https://developer.apple.com/documentation/apple_silicon/apple_m1}

  \item Intel. (2018). \emph{Intel UHD Graphics 620 — Product Specifications}. Intel ARK. \url{https://ark.intel.com/content/www/us/en/ark/products/126789}

  \item Apple. (2023). \emph{Apple M2 Max Chip — Technical Overview}. Apple Developer Documentation. \url{https://developer.apple.com/documentation/apple_silicon/apple_m2_max}

  \item MDN Web Docs. (2023). \emph{WebAssembly SIMD}. \url{https://developer.mozilla.org/en-US/docs/WebAssembly/SIMD}
\end{enumerate}


\newpage
\AddToContents{Bibliography}
\printbibliography
\end{document}